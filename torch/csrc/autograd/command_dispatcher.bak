#include <c10/cuda/CUDAGuard.h>

#include <torch/csrc/autograd/command_dispatcher.h>
#include <torch/csrc/jit/frontend/tracer.h>
#include <torch/csrc/jit/runtime/operator.h>

#include <iostream>
#include <map>
#include <sstream>
#include <stdexcept>
#include <string>
#include <utility>

class ConcurrentQueue {
 public:
  bool empty() const {
    return queue_.empty();
  }

  T pop() {
    std::unique_lock<std::mutex> mlock(mutex_);
    while (queue_.empty()) {
      cond_.wait(mlock);
    }
    auto val = queue_.front();
    queue_.pop();
    mlock.unlock();
    cond_.notify_one();
    return val;
  }

  void pop(T& item) {
    std::unique_lock<std::mutex> mlock(mutex_);
    while (queue_.empty()) {
      cond_.wait(mlock);
    }
    item = queue_.front();
    queue_.pop();
    mlock.unlock();
    cond_.notify_one();
  }

  void push(const T& item) {
    std::unique_lock<std::mutex> mlock(mutex_);
    queue_.push(item);
    mlock.unlock();
    cond_.notify_one();
  }
  ConcurrentQueue()=default;
  ConcurrentQueue(const ConcurrentQueue&) = delete;            // disable copying
  ConcurrentQueue& operator=(const ConcurrentQueue&) = delete; // disable assignment

 private:
  std::queue<T> queue_;
  std::mutex mutex_;
  std::condition_variable cond_;
};


namespace torch {
namespace autograd {
namespace profiler {

// const std::map<at::RecordScope, std::string> RecordScopeName = {
//     {at::RecordScope::FUNCTION, "FUNCTION"},
//     {at::RecordScope::BACKWARD_FUNCTION, "BACKWARD_FUNCTION"},
//     {at::RecordScope::TORCHSCRIPT_FUNCTION, "TORCHSCRIPT_FUNCTION"},
//     {at::RecordScope::KERNEL_FUNCTION_DTYPE, "KERNEL_FUNCTION_DTYPE"},
//     {at::RecordScope::USER_SCOPE, "USER_SCOPE"}};

std::map<void*, c10::StorageImpl*> offloadTensors;
// std::set<c10::intrusive_ptr<c10::TensorImpl>> offloadTensors;

CUDAStreamStub d2h_stream;
CUDAStreamStub h2d_stream;

struct CommandDispatcherData {
  std::vector<c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>>
      tensors;
  at::StringView name;

  CommandDispatcherData(const at::StringView& name_) : name(name_){};
};

void copyBytesCallBack(
    size_t nbytes,
    const void* src,
    c10::Device src_device,
    void* dst,
    c10::Device dst_device) {
  if (src_device.type() == c10::DeviceType::CPU &&
      dst_device.type() == c10::DeviceType::CUDA) {
    std::cerr << "H2D Memory Copy\n";
  } else if (
      src_device.type() == c10::DeviceType::CUDA &&
      dst_device.type() == c10::DeviceType::CPU) {
    std::cerr << "D2H Memory Copy\n";
  }
}

void returnAllTensors() {
  std::cerr << "returnAllTensors " << offloadTensors.size() << "\n";
  for (auto ptr : offloadTensors) {
  }
  offloadTensors.clear();
}

void prefetch(const at::RecordFunction& fn) {
  auto inputs = fn.inputs();
  auto data = new CommandDispatcherData(fn.name());

  for (int i = 0; i < inputs.size(); i++) {
    if (!inputs[i].isTensor())
      continue;

    at::Tensor& tensor = inputs[i].toTensor();
    if (tensor.defined() && tensor.has_storage() &&
        tensor.storage().nbytes() > 1e6) {
      auto original_data_ptr = tensor.data_ptr();
      // auto it = offloadTensors.find(tensor.data_ptr());
      // if (it != offloadTensors.end()) {
      // auto storage_impl_ = it->second;
      auto storage_impl_ = tensor.storage().unsafeGetStorageImpl();
      storage_impl_->swap_in();
      // offloadTensors.erase(it);

      // auto tensor_impl_ = tensor.getIntrusivePtr();
      // tensor_impl_->onDemendSwapIn();
      // }
      std::cerr << "Prefetch " << original_data_ptr << " to "
                << tensor.data_ptr() << " " << tensor.storage().device()
                << " size=" << tensor.storage().nbytes() << "\n";
    }
  }
}

void offload(const at::RecordFunction& fn) {
  if (fn.name() == at::StringView("aten::to") ||
      fn.name() == at::StringView("aten::copy_"))
    return;

  auto outputs = fn.outputs();
  auto data = new CommandDispatcherData(fn.name());

  if (outputs.size() > 0) {
    // auto event = cudaStubs()->registerComputeStreamEvent();
    for (int i = 0; i < outputs.size(); i++) {
      if (!outputs[i].isTensor())
        continue;

      at::Tensor& tensor = outputs[i].toTensor();
      if (tensor.defined() && tensor.device().is_cuda() &&
          tensor.has_storage() && tensor.storage().nbytes() > 1e6) {
        auto original_data_ptr = tensor.data_ptr();

        // cudaStubs()->issueStreamWaitEvent(event, d2h_stream);
        auto storage_impl_ = tensor.storage().unsafeGetStorageImpl();
        storage_impl_->swap_out(
            c10::Device(c10::DeviceType::CPU, 0), copyBytesCallBack);
        // offloadTensors.insert(std::make_pair(tensor.data_ptr(),
        // storage_impl_));

        // auto tensor_impl_ = tensor.getIntrusivePtr();
        // tensor_impl_->scheduleMemcopyAsync(
        //     c10::Device(c10::DeviceType::CPU, 0), copyBytesCallBack);

        // The space is allocated at this point, but the tranfer will happened
        // when the kernel is end.
        std::cerr << "Offload " << original_data_ptr << " to "
                  << tensor.data_ptr() << " " << tensor.storage().device()
                  << " size=" << tensor.storage().nbytes() << "\n";
      }
    }
  }
}

struct CommandDispatcherEvent {
  CUDAEventStub event;
  at::StringView name;
  int flag;
  int idx;

  CommandDispatcherEvent(int flag_): flag(flag_) {}

  CommandDispatcherEvent(
      CUDAEventStub event_,
      const at::StringView& name_,
      int flag_,
      int idx_)
      : event(event_),
        name(name_),
        flag(flag_),
        idx(idx_) {}
};

std::unique_ptr<std::thread> sync_thread;
std::unique_ptr<ConcurrentQueue<CommandDispatcherEvent>> event_queue;
std::atomic_bool sync_thread_stop_sinal;
int kernel_index;

void ComputeStreamSynchornizeThread() {
  while ( true ) {
    auto event = event_queue->pop();
    if (event.flag == 2) break;

    cudaStubs()->eventSynchronize(event.event);

    if (event.flag == 1)
      std::cerr << "Before (compute_stream_synchornize_thread) " << event.name
                << " " << (event.idx) << "\n";
    else 
      std::cerr << "After (compute_stream_synchornize_thread) " << event.name
                << " " << (event.idx) << "\n";
  }
}

void comandDispatcherFinalizer() {
  std::cerr << "comandDispatcherFinalizer \n";
  event_queue->push(CommandDispatcherEvent(2));
  sync_thread->join();
  std::cerr << "comandDispatcherFinalizer joined\n";
  // returnAllTensors();
  // stop_sync_thread = true;
  // auto stop_sync_thread_ = std::move(stop_sync_thread);
  // *stop_sync_thread_ = false;
  return;
}

void comandDispatcherInitializer() {
  // d2h_stream = cudaStubs()->createStream();
  // h2d_stream = cudaStubs()->createStream();
  std::cerr << "comandDispatcherInitializer \n";

  event_queue = std::move(std::make_unique<ConcurrentQueue<CommandDispatcherEvent>>());
  sync_thread_stop_sinal = true;
  sync_thread = std::move(std::make_unique<std::thread>(ComputeStreamSynchornizeThread));

  auto handle = at::addGlobalCallback(
      at::RecordFunctionCallback(
          [](const at::RecordFunction& fn)
              -> std::unique_ptr<at::ObserverContext> {
            at::RecordScope scope = fn.scope();
            std::cerr << "Before " << fn.name() << " " << (kernel_index)
                      << "\n";
            auto ctx_ptr = std::make_unique<CommandDispatcherObserverContext>();

            // prefetch(fn);
            event_queue->push(CommandDispatcherEvent(
                cudaStubs()->registerComputeStreamEvent(),
                fn.name(),
                1,
                kernel_index));
            return ctx_ptr;
          },
          [](const at::RecordFunction& fn, at::ObserverContext* ctx_ptr) {
            std::cerr << "After " << fn.name() << " " << (kernel_index)
                      << "\n";
            // offload(fn);
            event_queue->push(CommandDispatcherEvent(
                cudaStubs()->registerComputeStreamEvent(),
                fn.name(),
                0,
                kernel_index));

            kernel_index++;
          })
          .needsInputs(true)
          .needsOutputs(true)
          .needsIds(true));
}
} // namespace profiler
} // namespace autograd
} // namespace torch




std::unique_ptr<std::thread> prefetch_thread;
std::unique_ptr<std::thread> offload_thread;
std::unique_ptr<ConcurrentQueue<CommandDispatcherMemcpyCmd>>
    prefetch_command_queue;
std::unique_ptr<ConcurrentQueue<CommandDispatcherMemcpyCmd>>
    offload_command_queue;

std::mutex offloadTensorsMutex;
void prefetchThread() {
  while (true) {
    auto cmd = prefetch_command_queue->pop();
    if (cmd.flag == 2)
      break;

    if (cmd.kind == (int)cudaMemcpyHostToDevice) {
      std::cerr << "memcpyStreamSynchornizeThread command prefetch\n";
      offloadTensorsMutex.lock();
      auto it = offloadTensors.find(cmd.src);
      offloadTensorsMutex.unlock();

      if (it == offloadTensors.end()) {
        std::cerr << "Cannot find this tensor in offload tensors.\n";
      } else {
        cudaStubs()->streamWaitEvent(it->second, prefetch_stream);
        cudaStubs()->memcpyAsync(
            cmd.dst, cmd.src, cmd.nbytes, cmd.kind, prefetch_stream);

        offloadTensorsMutex.lock();
        offloadTensors.erase(it);
        offloadTensorsMutex.unlock();
      }
    }
  }
}
void offloadThread() {
  while (true) {
    auto cmd = offload_command_queue->pop();
    if (cmd.flag == 2)
      break;

    if (cmd.kind == (int)cudaMemcpyDeviceToHost) {
      std::cerr << "memcpyStreamSynchornizeThread command offload\n";
      cudaStubs()->memcpyAsync(
          cmd.dst, cmd.src, cmd.nbytes, cmd.kind, offload_stream);

      auto event = cudaStubs()->registerStreamEvent(offload_stream);
      offloadTensorsMutex.lock();
      offloadTensors.insert(make_pair(cmd.dst, event));
      offloadTensorsMutex.unlock();
    }
  }
}
void dummy_kernel(void* data) {}


struct CommandDispatcherData {
  std::vector<c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>>
      tensors;
  at::StringView name;

  CommandDispatcherData(const at::StringView& name_) : name(name_){};
};

struct CommandDispatcherMemcpyCmd {
  void* dst;
  const void* src;
  size_t nbytes;
  int kind;
  CUDAStreamStub stream;
  int flag = 1;

  CommandDispatcherMemcpyCmd(
      void* dst_,
      const void* src_,
      size_t nbytes_,
      int kind_,
      CUDAStreamStub stream_)
      : dst(dst_),
        src(src_),
        nbytes(nbytes_),
        kind(kind_),
        stream(stream_),
        flag(1) {}

  CommandDispatcherMemcpyCmd(int flag_) : flag(flag_) {}
};
